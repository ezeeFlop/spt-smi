# generated by datamodel-codegen:
#   filename:  openapi.json
#   timestamp: 2024-03-27T08:57:34+00:00

from __future__ import annotations

from enum import Enum
from typing import List, Optional, Union

from pydantic import AnyUrl, BaseModel, EmailStr, Field, confloat, conint, constr
from typing_extensions import Literal


class GenerationID(BaseModel):
    __root__: constr(min_length=64, max_length=64) = Field(
        ...,
        description='The `id` of a generation, typically used for async generations, that can be used to check the status of the generation or retrieve the result.',
        example='a6dc6c6e20acda010fe14d71f180658f2896ed9b4ec25aa99a6ff06c796987c4',
    )


class ImageToVideoRequest(BaseModel):
    image: bytes = Field(
        ...,
        description='The source image used in the video generation process. \nPlease ensure that the source image is in the correct format and dimensions.\n\nSupported Formats:\n- jpeg\n- png\n\nSupported Dimensions:\n- 1024x576\n- 576x1024\n- 768x768',
        example='./some/image.png',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description="A specific value that is used to guide the 'randomness' of the generation. (Omit this parameter or pass `0` to use a random seed.)",
    )
    cfg_scale: Optional[confloat(ge=0.0, le=10.0)] = Field(
        1.8,
        description='How strongly the video sticks to the original image. Use lower values to allow the model more freedom to make changes and higher values to correct motion distortions.',
    )
    motion_bucket_id: Optional[confloat(ge=1.0, le=255.0)] = Field(
        127,
        description='Lower values generally result in less motion in the output video, while higher values generally result in more motion. This parameter corresponds to the motion_bucket_id parameter from the [paper](https://static1.squarespace.com/static/6213c340453c3f502425776e/t/655ce779b9d47d342a93c890/1700587395994/stable_video_diffusion.pdf).',
    )


class Name(Enum):
    content_moderation = 'content_moderation'


class ContentModerationResponse(BaseModel):
    name: Name = Field(
        ...,
        description='Our content moderation system has flagged some part of your request and subsequently denied it.  You were not charged for this request.  While this may at times be frustrating, it is necessary to maintain the integrity of our platform and ensure a safe experience for all users.\n\nIf you would like to provide feedback, please use the [Support Form](https://platform.stability.ai/support) with the category "Feedback".',
    )
    errors: List[str] = Field(
        ...,
        description='One or more error messages. In most cases this will be an array with a single element, but sometimes there will be multiple messages.',
        example=['some-field: is required'],
        min_items=1,
    )


class Mode(Enum):
    search = 'search'


class OutputFormat(Enum):
    jpeg = 'jpeg'
    png = 'png'
    webp = 'webp'


class InpaintingSearchModeRequestBody(BaseModel):
    mode: Literal['search'] = Field(
        ...,
        description='Controls how the model decides which areas to inpaint and which areas to leave alone.  \n\nSpecifying `mask` requires:\n  - Provide an explicit mask image in the `mask` parameter\n  - Use the alpha channel of the `image` parameter as the mask\n  \nSpecifying `search` requires:\n  - Provide a small description of what to inpaint in the `search_prompt` parameter',
    )
    search_prompt: str = Field(
        ...,
        description='Short description of what to inpaint in the `image`.',
        example='glasses',
    )
    image: bytes = Field(
        ...,
        description='The image you wish to inpaint.\n\nSupported Formats:\n- jpeg\n- png\n- webp\n\nImage Dimensions:\n- Every side must be at least 64 pixels\n- The total pixel count cannot exceed 9,437,184 pixels (e.g. 3072x3072, 4096x2304, etc.)',
        example='./some/image.png',
    )
    prompt: constr(min_length=1) = Field(
        ...,
        description="What you wish to see in the output image. A strong, descriptive prompt that clearly defines \nelements, colors, and subjects will lead to better results. \n\nTo control the weight of a given word use the format `(word:weight)`, \nwhere `word` is the word you'd like to control the weight of and `weight` \nis a value between 0 and 1. For example: `The sky was a crisp (blue:0.3) and (green:0.8)`\nwould convey a sky that was blue and green, but more green than blue.",
    )
    negative_prompt: Optional[str] = Field(
        None,
        description='A blurb of text describing what you **do not** wish to see in the output image.  \nThis is an advanced feature.',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description="A specific value that is used to guide the 'randomness' of the generation. (Omit this parameter or pass `0` to use a random seed.)",
    )
    output_format: Optional[OutputFormat] = Field(
        'png', description='Dictates the `content-type` of the generated image.'
    )


class Mode1(Enum):
    mask = 'mask'


class InpaintingMaskingModeRequestBody(BaseModel):
    mode: Literal['mask'] = Field(
        ...,
        description='Controls how the model decides which areas to inpaint and which areas to leave alone.  \n\nSpecifying `mask` requires:\n  - Provide an explicit mask image in the `mask` parameter\n  - Use the alpha channel of the `image` parameter as the mask\n  \nSpecifying `search` requires:\n  - Provide a small description of what to inpaint in the `search_prompt` parameter',
    )
    mask: Optional[bytes] = Field(
        None,
        description="Controls the strength of the inpainting process on a per-pixel basis, either via a \nsecond image (passed into this parameter) or via the alpha channel of the `image` parameter.\n\n**Passing in a Mask**  \n\nThe image passed to this parameter should be a black and white image that represents, \nat any pixel, the strength of inpainting based on how dark or light the given pixel is. \nCompletely black pixels represent no inpainting strength while completely white pixels \nrepresent maximum strength.\n\nIn the event the mask is a different size than the `image` parameter, it will be automatically resized.\n\n**Alpha Channel Support**\n\nIf you don't provide an explicit mask, one will be derived from the alpha channel of the `image` parameter.\nTransparent pixels will be inpainted while opaque pixels will be preserved.\n\nIn the event an `image` with an alpha channel is provided along with a `mask`, the `mask` will take precedence.",
        example='./some/image.png',
    )
    image: bytes = Field(
        ...,
        description='The image you wish to inpaint.\n\nSupported Formats:\n- jpeg\n- png\n- webp\n\nImage Dimensions:\n- Every side must be at least 64 pixels\n- The total pixel count cannot exceed 9,437,184 pixels (e.g. 3072x3072, 4096x2304, etc.)',
        example='./some/image.png',
    )
    prompt: constr(min_length=1) = Field(
        ...,
        description="What you wish to see in the output image. A strong, descriptive prompt that clearly defines \nelements, colors, and subjects will lead to better results. \n\nTo control the weight of a given word use the format `(word:weight)`, \nwhere `word` is the word you'd like to control the weight of and `weight` \nis a value between 0 and 1. For example: `The sky was a crisp (blue:0.3) and (green:0.8)`\nwould convey a sky that was blue and green, but more green than blue.",
    )
    negative_prompt: Optional[str] = Field(
        None,
        description='A blurb of text describing what you **do not** wish to see in the output image.  \nThis is an advanced feature.',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description="A specific value that is used to guide the 'randomness' of the generation. (Omit this parameter or pass `0` to use a random seed.)",
    )
    output_format: Optional[OutputFormat] = Field(
        'png', description='Dictates the `content-type` of the generated image.'
    )


class OutpaintDirection(BaseModel):
    __root__: conint(ge=0, le=512) = Field(
        ...,
        description='The number of pixels to outpaint on the left side of the image. At least one outpainting direction must be supplied with a non-zero value.',
    )


class Type(Enum):
    AUDIO = 'AUDIO'
    CLASSIFICATION = 'CLASSIFICATION'
    PICTURE = 'PICTURE'
    STORAGE = 'STORAGE'
    TEXT = 'TEXT'
    VIDEO = 'VIDEO'


class Engine(BaseModel):
    description: str
    id: str = Field(
        ...,
        description='Unique identifier for the engine',
        example='stable-diffusion-v1-6',
    )
    name: str = Field(
        ..., description='Name of the engine', example='Stable Diffusion XL v1.0'
    )
    type: Type = Field(
        ..., description='The type of content this engine produces', example='PICTURE'
    )


class Error(BaseModel):
    id: str = Field(
        ...,
        description='A unique identifier for this particular occurrence of the problem.',
        example='296a972f-666a-44a1-a3df-c9c28a1f56c0',
    )
    name: str = Field(
        ...,
        description='The short-name of this class of errors e.g. `bad_request`.',
        example='bad_request',
    )
    message: str = Field(
        ...,
        description='A human-readable explanation specific to this occurrence of the problem.',
        example='Header parameter Authorization is required, but not found',
    )


class CfgScale(BaseModel):
    __root__: confloat(ge=0.0, le=35.0) = Field(
        ...,
        description='How strictly the diffusion process adheres to the prompt text (higher values keep your image closer to your prompt)',
        example=7,
    )


class ClipGuidancePreset(Enum):
    FAST_BLUE = 'FAST_BLUE'
    FAST_GREEN = 'FAST_GREEN'
    NONE = 'NONE'
    SIMPLE = 'SIMPLE'
    SLOW = 'SLOW'
    SLOWER = 'SLOWER'
    SLOWEST = 'SLOWEST'


class UpscaleImageHeight(BaseModel):
    __root__: conint(ge=512) = Field(
        ...,
        description='Desired height of the output image.  Only one of `width` or `height` may be specified.',
    )


class UpscaleImageWidth(BaseModel):
    __root__: conint(ge=512) = Field(
        ...,
        description='Desired width of the output image.  Only one of `width` or `height` may be specified.',
    )


class DiffuseImageHeight(BaseModel):
    __root__: conint(ge=128, multiple_of=64) = Field(
        ...,
        description='Height of the image to generate, in pixels, in an increment divisible by 64.',
        example=512,
    )


class DiffuseImageWidth(BaseModel):
    __root__: conint(ge=128, multiple_of=64) = Field(
        ...,
        description='Width of the image to generate, in pixels, in an increment divisible by 64.',
        example=512,
    )


class Sampler(Enum):
    DDIM = 'DDIM'
    DDPM = 'DDPM'
    K_DPMPP_2M = 'K_DPMPP_2M'
    K_DPMPP_2S_ANCESTRAL = 'K_DPMPP_2S_ANCESTRAL'
    K_DPM_2 = 'K_DPM_2'
    K_DPM_2_ANCESTRAL = 'K_DPM_2_ANCESTRAL'
    K_EULER = 'K_EULER'
    K_EULER_ANCESTRAL = 'K_EULER_ANCESTRAL'
    K_HEUN = 'K_HEUN'
    K_LMS = 'K_LMS'


class Samples(BaseModel):
    __root__: conint(ge=1, le=10) = Field(
        ..., description='Number of images to generate', example=1
    )


class Seed(BaseModel):
    __root__: conint(ge=0, le=4294967295) = Field(
        ...,
        description='Random noise seed (omit this option or use `0` for a random seed)',
        example=0,
    )


class Steps(BaseModel):
    __root__: conint(ge=10, le=50) = Field(
        ..., description='Number of diffusion steps to run.', example=50
    )


class Extras(BaseModel):
    pass


class StylePreset(Enum):
    enhance = 'enhance'
    anime = 'anime'
    photographic = 'photographic'
    digital_art = 'digital-art'
    comic_book = 'comic-book'
    fantasy_art = 'fantasy-art'
    line_art = 'line-art'
    analog_film = 'analog-film'
    neon_punk = 'neon-punk'
    isometric = 'isometric'
    low_poly = 'low-poly'
    origami = 'origami'
    modeling_compound = 'modeling-compound'
    cinematic = 'cinematic'
    field_3d_model = '3d-model'
    pixel_art = 'pixel-art'
    tile_texture = 'tile-texture'


class TextPrompt(BaseModel):
    text: constr(max_length=2000) = Field(
        ..., description='The prompt itself', example='A lighthouse on a cliff'
    )
    weight: Optional[float] = Field(
        None,
        description='Weight of the prompt (use negative numbers for negative prompts)',
        example=0.8167237,
    )


class TextPromptsForTextToImage(BaseModel):
    __root__: List[TextPrompt] = Field(
        ...,
        description='An array of text prompts to use for generation.\n\nGiven a text prompt with the text `A lighthouse on a cliff` and a weight of `0.5`, it would be represented as:\n\n```\n"text_prompts": [\n  {\n    "text": "A lighthouse on a cliff",\n    "weight": 0.5\n  }\n]\n```',
        min_items=1,
        title='TextPrompts',
    )


class TextPrompts(BaseModel):
    __root__: List[TextPrompt] = Field(
        ...,
        description='An array of text prompts to use for generation.\n\nDue to how arrays are represented in `multipart/form-data` requests, prompts must adhere to the format `text_prompts[index][text|weight]`,\nwhere `index` is some integer used to tie the text and weight together.  While `index` does not have to be sequential, duplicate entries \nwill override previous entries, so it is recommended to use sequential indices.\n\nGiven a text prompt with the text `A lighthouse on a cliff` and a weight of `0.5`, it would be represented as:\n```\ntext_prompts[0][text]: "A lighthouse on a cliff"\ntext_prompts[0][weight]: 0.5\n```\n\nTo add another prompt to that request simply provide the values under a new `index`:\n\n```\ntext_prompts[0][text]: "A lighthouse on a cliff"\ntext_prompts[0][weight]: 0.5\ntext_prompts[1][text]: "land, ground, dirt, grass"\ntext_prompts[1][weight]: -0.9\n```',
        min_items=1,
    )


class InputImage(BaseModel):
    __root__: bytes = Field(
        ..., description='The image to upscale using ESRGAN.', example='<image binary>'
    )


class InitImage(BaseModel):
    __root__: bytes = Field(
        ...,
        description='Image used to initialize the diffusion process, in lieu of random noise.',
        example='<image binary>',
    )


class InitImageStrength(BaseModel):
    __root__: confloat(ge=0.0, le=1.0) = Field(
        ...,
        description='How much influence the `init_image` has on the diffusion process. Values close to `1` will yield images very similar to the `init_image` while values close to `0` will yield images wildly different than the `init_image`. The behavior of this is meant to mirror DreamStudio\'s "Image Strength" slider.  <br/> <br/> This parameter is just an alternate way to set `step_schedule_start`, which is done via the calculation `1 - image_strength`. For example, passing in an Image Strength of 35% (`0.35`) would result in a `step_schedule_start` of `0.65`.\n',
        example=0.4,
    )


class InitImageMode(Enum):
    IMAGE_STRENGTH = 'IMAGE_STRENGTH'
    STEP_SCHEDULE = 'STEP_SCHEDULE'


class StepScheduleStart(BaseModel):
    __root__: confloat(ge=0.0, le=1.0) = Field(
        ...,
        description='Skips a proportion of the start of the diffusion steps, allowing the init_image to influence the final generated image.  Lower values will result in more influence from the init_image, while higher values will result in more influence from the diffusion steps.  (e.g. a value of `0` would simply return you the init_image, where a value of `1` would return you a completely different image.)',
        example=0.4,
    )


class StepScheduleEnd(BaseModel):
    __root__: confloat(ge=0.0, le=1.0) = Field(
        ...,
        description='Skips a proportion of the end of the diffusion steps, allowing the init_image to influence the final generated image.  Lower values will result in more influence from the init_image, while higher values will result in more influence from the diffusion steps.',
        example=0.01,
    )


class MaskImage(BaseModel):
    __root__: bytes = Field(
        ...,
        description='Optional grayscale mask that allows for influence over which pixels are eligible for diffusion and at what strength. Must be the same dimensions as the `init_image`. Use the `mask_source` option to specify whether the white or black pixels should be inpainted.',
        example='<image binary>',
    )


class MaskSource(BaseModel):
    __root__: str = Field(
        ...,
        description='For any given pixel, the mask determines the strength of generation on a linear scale.  This parameter determines where to source the mask from:\n- `MASK_IMAGE_WHITE` will use the white pixels of the mask_image as the mask, where white pixels are completely replaced and black pixels are unchanged\n- `MASK_IMAGE_BLACK` will use the black pixels of the mask_image as the mask, where black pixels are completely replaced and white pixels are unchanged\n- `INIT_IMAGE_ALPHA` will use the alpha channel of the init_image as the mask, where fully transparent pixels are completely replaced and fully opaque pixels are unchanged',
    )


class GenerationRequestOptionalParams(BaseModel):
    cfg_scale: Optional[CfgScale] = None
    clip_guidance_preset: Optional[ClipGuidancePreset] = 'NONE'
    sampler: Optional[Sampler] = None
    samples: Optional[Samples] = None
    seed: Optional[Seed] = None
    steps: Optional[Steps] = None
    style_preset: Optional[StylePreset] = None
    extras: Optional[Extras] = None


class RealESRGANUpscaleRequestBody(BaseModel):
    image: InputImage
    width: Optional[UpscaleImageWidth] = None
    height: Optional[UpscaleImageHeight] = None


class ImageToImageRequestBody(BaseModel):
    text_prompts: TextPrompts
    init_image: InitImage
    init_image_mode: Optional[InitImageMode] = 'IMAGE_STRENGTH'
    image_strength: Optional[InitImageStrength] = None
    step_schedule_start: Optional[StepScheduleStart] = None
    step_schedule_end: Optional[StepScheduleEnd] = None
    cfg_scale: Optional[CfgScale] = None
    clip_guidance_preset: Optional[ClipGuidancePreset] = 'NONE'
    sampler: Optional[Sampler] = None
    samples: Optional[Samples] = None
    seed: Optional[Seed] = None
    steps: Optional[Steps] = None
    style_preset: Optional[StylePreset] = None
    extras: Optional[Extras] = None


class ImageToImageUsingImageStrengthRequestBody(GenerationRequestOptionalParams):
    text_prompts: TextPrompts
    init_image: InitImage
    init_image_mode: Optional[InitImageMode] = 'IMAGE_STRENGTH'
    image_strength: Optional[InitImageStrength] = None


class ImageToImageUsingStepScheduleRequestBody(GenerationRequestOptionalParams):
    text_prompts: TextPrompts
    init_image: InitImage
    init_image_mode: Optional[InitImageMode] = 'IMAGE_STRENGTH'
    step_schedule_start: Optional[StepScheduleStart] = None
    step_schedule_end: Optional[StepScheduleEnd] = None


class MaskingRequestBody(BaseModel):
    init_image: InitImage
    mask_source: MaskSource
    mask_image: Optional[MaskImage] = None
    text_prompts: TextPrompts
    cfg_scale: Optional[CfgScale] = None
    clip_guidance_preset: Optional[ClipGuidancePreset] = 'NONE'
    sampler: Optional[Sampler] = None
    samples: Optional[Samples] = None
    seed: Optional[Seed] = None
    steps: Optional[Steps] = None
    style_preset: Optional[StylePreset] = None
    extras: Optional[Extras] = None


class MaskingUsingMaskImageRequestBody(GenerationRequestOptionalParams):
    text_prompts: TextPrompts
    init_image: InitImage
    mask_source: MaskSource
    mask_image: MaskImage


class MaskingUsingInitImageAlphaRequestBody(GenerationRequestOptionalParams):
    text_prompts: TextPrompts
    init_image: InitImage
    mask_source: MaskSource


class TextToImageRequestBody(GenerationRequestOptionalParams):
    height: Optional[DiffuseImageHeight] = None
    width: Optional[DiffuseImageWidth] = None
    text_prompts: TextPromptsForTextToImage


class BalanceResponseBody(BaseModel):
    credits: float = Field(
        ...,
        description='The balance of the account/organization associated with the API key',
        example=0.41122252265928866,
    )


class ListEnginesResponseBody(BaseModel):
    __root__: List[Engine] = Field(
        ...,
        description='The engines available to your user/organization',
        example=[
            {
                'description': 'Stability-AI Stable Diffusion v1.6',
                'id': 'stable-diffusion-v1-6',
                'name': 'Stable Diffusion v1.6',
                'type': 'PICTURE',
            },
            {
                'description': 'Stability-AI Stable Diffusion XL v1.0',
                'id': 'stable-diffusion-xl-1024-v1-0',
                'name': 'Stable Diffusion XL v1.0',
                'type': 'PICTURE',
            },
        ],
    )


class FinishReason(Enum):
    SUCCESS = 'SUCCESS'
    ERROR = 'ERROR'
    CONTENT_FILTERED = 'CONTENT_FILTERED'


class Image(BaseModel):
    base64: Optional[str] = Field(None, description='Image encoded in base64')
    finishReason: Optional[FinishReason] = Field(None, example='CONTENT_FILTERED')
    seed: Optional[float] = Field(
        None, description='The seed associated with this image', example=1229191277
    )


class OrganizationMembership(BaseModel):
    id: str = Field(..., example='org-123456')
    is_default: bool = Field(..., example=False)
    name: str = Field(..., example='My Organization')
    role: str = Field(..., example='MEMBER')


class V2alphaGenerationImageToVideoPostResponse(BaseModel):
    id: GenerationID


class V2alphaGenerationImageToVideoPostResponse1(BaseModel):
    name: constr(min_length=1) = Field(
        ...,
        description='Short-hand name for an error, useful for discriminating between errors with the same status code.',
        example='bad_request',
    )
    errors: List[str] = Field(
        ...,
        description='One or more error messages. In most cases this will be an array with a single element, but sometimes there will be multiple messages.',
        example=['some-field: is required'],
        min_items=1,
    )


class FinishReason2(Enum):
    SUCCESS = 'SUCCESS'
    CONTENT_FILTERED = 'CONTENT_FILTERED'


class V2alphaGenerationImageToVideoResultIdGetResponse(BaseModel):
    video: str = Field(
        ...,
        description='The generated video, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and one or more frames have been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class Status(Enum):
    in_progress = 'in-progress'


class V2alphaGenerationImageToVideoResultIdGetResponse1(BaseModel):
    id: GenerationID
    status: Status = Field(..., description='The status of your generation.')


class V2alphaGenerationImageToVideoResultIdGetResponse2(BaseModel):
    name: constr(min_length=1) = Field(
        ...,
        description='Short-hand name for an error, useful for discriminating between errors with the same status code.',
        example='bad_request',
    )
    errors: List[str] = Field(
        ...,
        description='One or more error messages. In most cases this will be an array with a single element, but sometimes there will be multiple messages.',
        example=['some-field: is required'],
        min_items=1,
    )


class V2alphaGenerationStableImageUpscalePostRequest(BaseModel):
    image: bytes = Field(
        ...,
        description='The image you wish to upscale.\n\nSupported Formats:\n- jpeg\n- png\n- webp\n\nSupported Sizes:\n- Minimum: 4,096 pixels (e.g. 64x64)\n- Maximum: 1,048,576 pixels (e.g. 1024x1024)',
        example='./some/image.png',
    )
    prompt: constr(min_length=1) = Field(
        ...,
        description="What you wish to see in the output image. A strong, descriptive prompt that clearly defines \nelements, colors, and subjects will lead to better results. \n\nTo control the weight of a given word use the format `(word:weight)`, \nwhere `word` is the word you'd like to control the weight of and `weight` \nis a value between 0 and 1. For example: `The sky was a crisp (blue:0.3) and (green:0.8)`\nwould convey a sky that was blue and green, but more green than blue.",
    )
    negative_prompt: Optional[str] = Field(
        None,
        description='A blurb of text describing what you **do not** wish to see in the output image.  \nThis is an advanced feature.',
    )
    output_format: Optional[OutputFormat] = Field(
        'png', description='Dictates the `content-type` of the generated image.'
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description="A specific value that is used to guide the 'randomness' of the generation. (Omit this parameter or pass `0` to use a random seed.)",
    )
    creativity: Optional[confloat(ge=0.0, le=0.35)] = Field(
        0.3,
        description='Indicates how creative the model should be when upscaling an image.\nHigher values will result in more details being added to the image during upscaling.',
    )


class V2alphaGenerationStableImageUpscalePostResponse(BaseModel):
    id: GenerationID


class V2alphaGenerationStableImageUpscalePostResponse1(BaseModel):
    name: constr(min_length=1) = Field(
        ...,
        description='Short-hand name for an error, useful for discriminating between errors with the same status code.',
        example='bad_request',
    )
    errors: List[str] = Field(
        ...,
        description='One or more error messages. In most cases this will be an array with a single element, but sometimes there will be multiple messages.',
        example=['some-field: is required'],
        min_items=1,
    )


class V2alphaGenerationStableImageUpscaleResultIdGetResponse(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2alphaGenerationStableImageUpscaleResultIdGetResponse1(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2alphaGenerationStableImageUpscaleResultIdGetResponse2(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2alphaGenerationStableImageUpscaleResultIdGetResponse3(BaseModel):
    id: GenerationID
    status: Status = Field(..., description='The status of your generation.')


class V2alphaGenerationStableImageUpscaleResultIdGetResponse4(BaseModel):
    name: constr(min_length=1) = Field(
        ...,
        description='Short-hand name for an error, useful for discriminating between errors with the same status code.',
        example='bad_request',
    )
    errors: List[str] = Field(
        ...,
        description='One or more error messages. In most cases this will be an array with a single element, but sometimes there will be multiple messages.',
        example=['some-field: is required'],
        min_items=1,
    )


class V2alphaGenerationStableImageInpaintPostRequest(BaseModel):
    __root__: Union[
        InpaintingSearchModeRequestBody, InpaintingMaskingModeRequestBody
    ] = Field(..., discriminator='mode')


class V2alphaGenerationStableImageInpaintPostResponse(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2alphaGenerationStableImageInpaintPostResponse1(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2alphaGenerationStableImageInpaintPostResponse2(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2alphaGenerationStableImageInpaintPostResponse3(BaseModel):
    name: constr(min_length=1) = Field(
        ...,
        description='Short-hand name for an error, useful for discriminating between errors with the same status code.',
        example='bad_request',
    )
    errors: List[str] = Field(
        ...,
        description='One or more error messages. In most cases this will be an array with a single element, but sometimes there will be multiple messages.',
        example=['some-field: is required'],
        min_items=1,
    )


class V2betaImageToVideoPostRequest(ImageToVideoRequest):
    pass


class V2betaImageToVideoPostResponse(BaseModel):
    id: GenerationID


class V2betaImageToVideoPostResponse1(BaseModel):
    name: constr(min_length=1) = Field(
        ...,
        description='Short-hand name for an error, useful for discriminating between errors with the same status code.',
        example='bad_request',
    )
    errors: List[str] = Field(
        ...,
        description='One or more error messages. In most cases this will be an array with a single element, but sometimes there will be multiple messages.',
        example=['some-field: is required'],
        min_items=1,
    )


class V2betaImageToVideoResultIdGetResponse(BaseModel):
    video: str = Field(
        ...,
        description='The generated video, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and one or more frames have been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2betaImageToVideoResultIdGetResponse1(BaseModel):
    id: GenerationID
    status: Status = Field(..., description='The status of your generation.')


class V2betaImageToVideoResultIdGetResponse2(BaseModel):
    name: constr(min_length=1) = Field(
        ...,
        description='Short-hand name for an error, useful for discriminating between errors with the same status code.',
        example='bad_request',
    )
    errors: List[str] = Field(
        ...,
        description='One or more error messages. In most cases this will be an array with a single element, but sometimes there will be multiple messages.',
        example=['some-field: is required'],
        min_items=1,
    )


class V2betaStableImageUpscaleCreativePostRequest(BaseModel):
    image: bytes = Field(
        ...,
        description='The image you wish to upscale.\n\nSupported Formats:\n- jpeg\n- png\n- webp\n\nSupported Sizes:\n- Every side must be at least 64 pixels\n- The total pixel count cannot exceed 1,048,576 pixels (e.g. 1024*1024, 2048*512, etc.)',
        example='./some/image.png',
    )
    prompt: constr(min_length=1) = Field(
        ...,
        description="What you wish to see in the output image. A strong, descriptive prompt that clearly defines \nelements, colors, and subjects will lead to better results. \n\nTo control the weight of a given word use the format `(word:weight)`, \nwhere `word` is the word you'd like to control the weight of and `weight` \nis a value between 0 and 1. For example: `The sky was a crisp (blue:0.3) and (green:0.8)`\nwould convey a sky that was blue and green, but more green than blue.",
    )
    negative_prompt: Optional[str] = Field(
        None,
        description='A blurb of text describing what you **do not** wish to see in the output image.  \nThis is an advanced feature.',
    )
    output_format: Optional[OutputFormat] = Field(
        'png', description='Dictates the `content-type` of the generated image.'
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description="A specific value that is used to guide the 'randomness' of the generation. (Omit this parameter or pass `0` to use a random seed.)",
    )
    creativity: Optional[confloat(ge=0.0, le=0.35)] = Field(
        0.3,
        description='Indicates how creative the model should be when upscaling an image.\nHigher values will result in more details being added to the image during upscaling.',
    )


class V2betaStableImageUpscaleCreativePostResponse(BaseModel):
    id: GenerationID


class V2betaStableImageUpscaleCreativePostResponse1(BaseModel):
    name: constr(min_length=1) = Field(
        ...,
        description='Short-hand name for an error, useful for discriminating between errors with the same status code.',
        example='bad_request',
    )
    errors: List[str] = Field(
        ...,
        description='One or more error messages. In most cases this will be an array with a single element, but sometimes there will be multiple messages.',
        example=['some-field: is required'],
        min_items=1,
    )


class V2betaStableImageUpscaleCreativeResultIdGetResponse(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2betaStableImageUpscaleCreativeResultIdGetResponse1(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2betaStableImageUpscaleCreativeResultIdGetResponse2(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2betaStableImageUpscaleCreativeResultIdGetResponse3(BaseModel):
    id: GenerationID
    status: Status = Field(..., description='The status of your generation.')


class V2betaStableImageUpscaleCreativeResultIdGetResponse4(BaseModel):
    name: constr(min_length=1) = Field(
        ...,
        description='Short-hand name for an error, useful for discriminating between errors with the same status code.',
        example='bad_request',
    )
    errors: List[str] = Field(
        ...,
        description='One or more error messages. In most cases this will be an array with a single element, but sometimes there will be multiple messages.',
        example=['some-field: is required'],
        min_items=1,
    )


class V2betaStableImageEditInpaintPostRequest(BaseModel):
    image: bytes = Field(
        ...,
        description='The image you wish to inpaint.\n\nSupported Formats:\n- jpeg\n- png\n- webp\n\nImage Dimensions:\n- Every side must be at least 64 pixels\n- The total pixel count cannot exceed 9,437,184 pixels (e.g. 3072x3072, 4096x2304, etc.)',
        example='./some/image.png',
    )
    prompt: constr(min_length=1) = Field(
        ...,
        description="What you wish to see in the output image. A strong, descriptive prompt that clearly defines \nelements, colors, and subjects will lead to better results. \n\nTo control the weight of a given word use the format `(word:weight)`, \nwhere `word` is the word you'd like to control the weight of and `weight` \nis a value between 0 and 1. For example: `The sky was a crisp (blue:0.3) and (green:0.8)`\nwould convey a sky that was blue and green, but more green than blue.",
    )
    negative_prompt: Optional[str] = Field(
        None,
        description='A blurb of text describing what you **do not** wish to see in the output image.  \nThis is an advanced feature.',
    )
    mask: Optional[bytes] = Field(
        None,
        description="Controls the strength of the inpainting process on a per-pixel basis, either via a \nsecond image (passed into this parameter) or via the alpha channel of the `image` parameter.\n\n**Passing in a Mask**  \n\nThe image passed to this parameter should be a black and white image that represents, \nat any pixel, the strength of inpainting based on how dark or light the given pixel is. \nCompletely black pixels represent no inpainting strength while completely white pixels \nrepresent maximum strength.\n\nIn the event the mask is a different size than the `image` parameter, it will be automatically resized.\n\n**Alpha Channel Support**\n\nIf you don't provide an explicit mask, one will be derived from the alpha channel of the `image` parameter.\nTransparent pixels will be inpainted while opaque pixels will be preserved.\n\nIn the event an `image` with an alpha channel is provided along with a `mask`, the `mask` will take precedence.",
        example='./some/image.png',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description="A specific value that is used to guide the 'randomness' of the generation. (Omit this parameter or pass `0` to use a random seed.)",
    )
    output_format: Optional[OutputFormat] = Field(
        'png', description='Dictates the `content-type` of the generated image.'
    )


class V2betaStableImageEditInpaintPostResponse(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2betaStableImageEditInpaintPostResponse1(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2betaStableImageEditInpaintPostResponse2(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2betaStableImageEditInpaintPostResponse3(BaseModel):
    name: constr(min_length=1) = Field(
        ...,
        description='Short-hand name for an error, useful for discriminating between errors with the same status code.',
        example='bad_request',
    )
    errors: List[str] = Field(
        ...,
        description='One or more error messages. In most cases this will be an array with a single element, but sometimes there will be multiple messages.',
        example=['some-field: is required'],
        min_items=1,
    )


class OutputFormat5(Enum):
    png = 'png'
    jpeg = 'jpeg'
    webp = 'webp'


class V2betaStableImageEditOutpaintPostRequest(BaseModel):
    image: bytes = Field(
        ...,
        description='The image you wish to inpaint.\n\nSupported Formats:\n- jpeg\n- png\n- webp\n\nImage Dimensions:\n- Every side must be at least 64 pixels\n- The total pixel count cannot exceed 9,437,184 pixels (e.g. 3072x3072, 4096x2304, etc.)',
        example='./some/image.png',
    )
    left: Optional[OutpaintDirection] = None
    right: Optional[OutpaintDirection] = None
    up: Optional[OutpaintDirection] = None
    down: Optional[OutpaintDirection] = None
    prompt: Optional[constr(min_length=0)] = Field(
        None,
        description="What you wish to see in the output image. A strong, descriptive prompt that clearly defines \nelements, colors, and subjects will lead to better results. \n\nTo control the weight of a given word use the format `(word:weight)`, \nwhere `word` is the word you'd like to control the weight of and `weight` \nis a value between 0 and 1. For example: `The sky was a crisp (blue:0.3) and (green:0.8)`\nwould convey a sky that was blue and green, but more green than blue.",
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description="A specific value that is used to guide the 'randomness' of the generation. (Omit this parameter or pass `0` to use a random seed.)",
    )
    output_format: Optional[OutputFormat5] = Field(
        'png', description='Dictates the `content-type` of the generated image.'
    )


class V2betaStableImageEditOutpaintPostResponse(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2betaStableImageEditOutpaintPostResponse1(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2betaStableImageEditOutpaintPostResponse2(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2betaStableImageEditOutpaintPostResponse3(BaseModel):
    name: constr(min_length=1) = Field(
        ...,
        description='Short-hand name for an error, useful for discriminating between errors with the same status code.',
        example='bad_request',
    )
    errors: List[str] = Field(
        ...,
        description='One or more error messages. In most cases this will be an array with a single element, but sometimes there will be multiple messages.',
        example=['some-field: is required'],
        min_items=1,
    )


class OutputFormat6(Enum):
    jpeg = 'jpeg'
    png = 'png'
    webp = 'webp'


class V2betaStableImageEditSearchAndReplacePostRequest(BaseModel):
    image: bytes = Field(
        ...,
        description='The image you wish to inpaint.\n\nSupported Formats:\n- jpeg\n- png\n- webp\n\nImage Dimensions:\n- Every side must be at least 64 pixels\n- The total pixel count cannot exceed 9,437,184 pixels (e.g. 3072x3072, 4096x2304, etc.)',
        example='./some/image.png',
    )
    prompt: constr(min_length=1) = Field(
        ...,
        description="What you wish to see in the output image. A strong, descriptive prompt that clearly defines \nelements, colors, and subjects will lead to better results. \n\nTo control the weight of a given word use the format `(word:weight)`, \nwhere `word` is the word you'd like to control the weight of and `weight` \nis a value between 0 and 1. For example: `The sky was a crisp (blue:0.3) and (green:0.8)`\nwould convey a sky that was blue and green, but more green than blue.",
    )
    search_prompt: str = Field(
        ...,
        description='Short description of what to inpaint in the `image`.',
        example='glasses',
    )
    negative_prompt: Optional[str] = Field(
        None,
        description='A blurb of text describing what you **do not** wish to see in the output image.  \nThis is an advanced feature.',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description="A specific value that is used to guide the 'randomness' of the generation. (Omit this parameter or pass `0` to use a random seed.)",
    )
    output_format: Optional[OutputFormat6] = Field(
        'png', description='Dictates the `content-type` of the generated image.'
    )


class V2betaStableImageEditSearchAndReplacePostResponse(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2betaStableImageEditSearchAndReplacePostResponse1(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2betaStableImageEditSearchAndReplacePostResponse2(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2betaStableImageEditSearchAndReplacePostResponse3(BaseModel):
    name: constr(min_length=1) = Field(
        ...,
        description='Short-hand name for an error, useful for discriminating between errors with the same status code.',
        example='bad_request',
    )
    errors: List[str] = Field(
        ...,
        description='One or more error messages. In most cases this will be an array with a single element, but sometimes there will be multiple messages.',
        example=['some-field: is required'],
        min_items=1,
    )


class OutputFormat7(Enum):
    png = 'png'
    webp = 'webp'


class V2betaStableImageEditRemoveBackgroundPostRequest(BaseModel):
    image: bytes = Field(
        ...,
        description='The image whose background you wish to remove.\nPlease ensure that the source image is in the correct format and dimensions.\n\nSupported Formats:\n- jpeg\n- png\n- webp\n\nSupported Sizes:\n- Every side must be at least 64 pixels\n- The total pixel count cannot exceed 4,194,304 pixels (e.g. 2048*2048, 4096*1024, etc.)',
        example='./some/image.png',
    )
    output_format: Optional[OutputFormat7] = Field(
        'png', description='Dictates the `content-type` of the generated image.'
    )


class V2betaStableImageEditRemoveBackgroundPostResponse(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2betaStableImageEditRemoveBackgroundPostResponse1(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2betaStableImageEditRemoveBackgroundPostResponse2(BaseModel):
    name: constr(min_length=1) = Field(
        ...,
        description='Short-hand name for an error, useful for discriminating between errors with the same status code.',
        example='bad_request',
    )
    errors: List[str] = Field(
        ...,
        description='One or more error messages. In most cases this will be an array with a single element, but sometimes there will be multiple messages.',
        example=['some-field: is required'],
        min_items=1,
    )


class AspectRatio(Enum):
    field_21_9 = '21:9'
    field_16_9 = '16:9'
    field_3_2 = '3:2'
    field_5_4 = '5:4'
    field_1_1 = '1:1'
    field_4_5 = '4:5'
    field_2_3 = '2:3'
    field_9_16 = '9:16'
    field_9_21 = '9:21'


class OutputFormat8(Enum):
    png = 'png'
    jpeg = 'jpeg'
    webp = 'webp'


class V2betaStableImageGenerateCorePostRequest(BaseModel):
    prompt: constr(min_length=1) = Field(
        ...,
        description="What you wish to see in the output image. A strong, descriptive prompt that clearly defines \nelements, colors, and subjects will lead to better results. \n\nTo control the weight of a given word use the format `(word:weight)`, \nwhere `word` is the word you'd like to control the weight of and `weight` \nis a value between 0 and 1. For example: `The sky was a crisp (blue:0.3) and (green:0.8)`\nwould convey a sky that was blue and green, but more green than blue.",
    )
    aspect_ratio: Optional[AspectRatio] = Field(
        '1:1', description='The aspect ratio of the generated image.'
    )
    negative_prompt: Optional[str] = Field(
        None,
        description='A blurb of text describing what you **do not** wish to see in the output image.  \nThis is an advanced feature.',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description="A specific value that is used to guide the 'randomness' of the generation. (Omit this parameter or pass `0` to use a random seed.)",
    )
    output_format: Optional[OutputFormat8] = Field(
        'png', description='Dictates the `content-type` of the generated image.'
    )


class V2betaStableImageGenerateCorePostResponse(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2betaStableImageGenerateCorePostResponse1(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2betaStableImageGenerateCorePostResponse2(BaseModel):
    image: str = Field(
        ...,
        description='The generated image, encoded to base64.',
        example='AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1...',
    )
    finish_reason: FinishReason2 = Field(
        ...,
        description='The reason the generation finished.\n\n- `SUCCESS` = successful generation.\n- `CONTENT_FILTERED` = successful generation, however the output violated our content moderation \npolicy and has been blurred as a result.',
        example='SUCCESS',
    )
    seed: Optional[confloat(ge=0.0, le=4294967294.0)] = Field(
        0,
        description='The seed used as random noise for this generation.',
        example=343940597,
    )


class V2betaStableImageGenerateCorePostResponse3(BaseModel):
    name: constr(min_length=1) = Field(
        ...,
        description='Short-hand name for an error, useful for discriminating between errors with the same status code.',
        example='bad_request',
    )
    errors: List[str] = Field(
        ...,
        description='One or more error messages. In most cases this will be an array with a single element, but sometimes there will be multiple messages.',
        example=['some-field: is required'],
        min_items=1,
    )


class V1GenerationEngineIdTextToImagePostResponse(BaseModel):
    artifacts: Optional[List[Image]] = None


class V1GenerationEngineIdImageToImagePostResponse(BaseModel):
    artifacts: Optional[List[Image]] = None


class V1GenerationEngineIdImageToImageMaskingPostResponse(BaseModel):
    artifacts: Optional[List[Image]] = None


class V1GenerationEngineIdImageToImageUpscalePostResponse(BaseModel):
    artifacts: Optional[List[Image]] = None


class AccountResponseBody(BaseModel):
    email: EmailStr = Field(
        ..., description="The user's email", example='example@stability.ai'
    )
    id: str = Field(..., description="The user's ID", example='user-1234')
    organizations: List[OrganizationMembership] = Field(
        ...,
        description="The user's organizations",
        example=[
            {
                'id': 'org-5678',
                'name': 'Another Organization',
                'role': 'MEMBER',
                'is_default': True,
            },
            {
                'id': 'org-1234',
                'name': 'My Organization',
                'role': 'MEMBER',
                'is_default': False,
            },
        ],
    )
    profile_picture: Optional[AnyUrl] = Field(
        None,
        description="The user's profile picture",
        example='https://api.stability.ai/example.png',
    )
